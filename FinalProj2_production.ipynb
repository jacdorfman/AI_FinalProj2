{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e2bafd",
   "metadata": {},
   "source": [
    "# Production stage: use files 0-6 for model traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93369536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_gathered_at_0.csv\t   data_gathered_at_3.csv  initial_training_data.csv\r\n",
      "data_gathered_at_1.csv\t   data_gathered_at_4.csv  test_data.csv\r\n",
      "data_gathered_at_2.csv\t   data_gathered_at_5.csv  validation_test_data.csv\r\n",
      "data_gathered_at_2_bk.csv  data_gathered_at_6.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91e1bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘accum_data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir accum_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "116df2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp raw_data/initial_training_data.csv accum_data/accumulated_data.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d199dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2620\r\n",
      "-rw-r--r-- 1 root root 2679360 May 30 12:42 accumulated_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l accum_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6acf5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "raw_data_path = 'raw_data/'\n",
    "accum_data_path = 'accum_data/'\n",
    "accum_file_name = accum_data_path+'accumulated_data.csv'\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\" Prepare data for modeling \n",
    "        input: data frame with labels und pixel data\n",
    "        output: image and label array \"\"\"\n",
    "    \n",
    "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
    "    image_label = np.array(list(map(int, data['Emotion'])))\n",
    "    \n",
    "    for i, row in enumerate(data.index):\n",
    "        image = np.fromstring(data.loc[row, 'Pixels'], dtype=int, sep=' ')\n",
    "        image = np.reshape(image, (48, 48))\n",
    "        image_array[i] = image\n",
    "        \n",
    "    return image_array, image_label\n",
    "\n",
    "\n",
    "def data_to_tf_data(df):\n",
    "    image_array, image_label = prepare_data(df)\n",
    "    images = image_array.reshape((image_array.shape[0], 48, 48, 1))\n",
    "    images = images.astype('float32')/255\n",
    "    labels = to_categorical(image_label)\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e171f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_accum_data(nmb):\n",
    "    accum_data = pd.read_csv(accum_file_name)\n",
    "    next_data_file = raw_data_path+'data_gathered_at_' + str(nmb)+'.csv'\n",
    "    add_data = pd.read_csv(next_data_file)\n",
    "    accum_data = accum_data.append(add_data, ignore_index = True)\n",
    "    accum_data.to_csv(accum_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72f4fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmb = 3\n",
    "add_to_accum_data(nmb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e257b6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 0.3013 - accuracy: 0.2507 - val_loss: 1.9419 - val_accuracy: 0.2449\n",
      "Epoch 2/12\n",
      "12/12 [==============================] - 1s 131ms/step - loss: 0.2944 - accuracy: 0.2587 - val_loss: 1.8995 - val_accuracy: 0.2449\n",
      "Epoch 3/12\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 0.2919 - accuracy: 0.2587 - val_loss: 1.8407 - val_accuracy: 0.2449\n",
      "Epoch 4/12\n",
      "12/12 [==============================] - 1s 131ms/step - loss: 0.2918 - accuracy: 0.2587 - val_loss: 1.8726 - val_accuracy: 0.2449\n",
      "Epoch 5/12\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 0.2871 - accuracy: 0.2587 - val_loss: 1.9259 - val_accuracy: 0.2449\n",
      "Epoch 6/12\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 0.2891 - accuracy: 0.2587 - val_loss: 1.8315 - val_accuracy: 0.2449\n",
      "Epoch 7/12\n",
      "12/12 [==============================] - 1s 131ms/step - loss: 0.2838 - accuracy: 0.2587 - val_loss: 1.8732 - val_accuracy: 0.2449\n",
      "Epoch 8/12\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 0.2809 - accuracy: 0.2601 - val_loss: 1.8355 - val_accuracy: 0.2502\n",
      "Epoch 9/12\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 0.2758 - accuracy: 0.2895 - val_loss: 1.8295 - val_accuracy: 0.2524\n",
      "Epoch 10/12\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 0.2673 - accuracy: 0.3043 - val_loss: 2.0705 - val_accuracy: 0.2683\n",
      "Epoch 11/12\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 0.2693 - accuracy: 0.3257 - val_loss: 1.8080 - val_accuracy: 0.2848\n",
      "Epoch 12/12\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 0.2561 - accuracy: 0.3365 - val_loss: 1.8537 - val_accuracy: 0.3054\n",
      "113/113 [==============================] - 1s 8ms/step - loss: 1.8611 - accuracy: 0.2867\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(accum_file_name)\n",
    "train_images, train_labels = data_to_tf_data(train)\n",
    "\n",
    "val = pd.read_csv(raw_data_path+'validation_test_data.csv')\n",
    "val_images, val_labels = data_to_tf_data(val)\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "class_weight = dict(zip(range(0, 7), (((train['Emotion'].value_counts()).sort_index())/len(train['Emotion'])).tolist()))\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    class_weight = class_weight,\n",
    "                    epochs=12,\n",
    "                    batch_size=64)\n",
    "\n",
    "df = pd.read_csv(raw_data_path+'test_data.csv')\n",
    "test_images, test_labels = data_to_tf_data(df)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "192c0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: initial_model/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model.save(\"initial_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe6a4414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "Computing file/dir hashes (only done once)            |0.00 [00:00,      ?md5/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "                                                      |0.00 [00:00,       ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Saving files                          0/4 [00:00<?,     ?file/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |.XRFJRWKC4MejeoWrDoYgDv.tmp    0.00/17.6k [00:00<?,       ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |.PmTTG2hiqwatfP5ZscMZsJ.tmp     0.00/166k [00:00<?,       ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |.3KkVYzZ9hsNJt7Qsa37dt6.tmp    0.00/3.83M [00:00<?,       ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |.TDNhbZL3fZU2E6kJNERVYA.tmp    0.00/2.72k [00:00<?,       ?it/s]\u001b[A\n",
      "100% Add|██████████████████████████████████████████████|1/1 [00:00,  2.51file/s]\u001b[A\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add initial_model.dvc\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add initial_model/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3e3e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add initial_model.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbb02d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is ahead of 'origin/master' by 6 commits.\r\n",
      "  (use \"git push\" to publish your local commits)\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n",
      "\r\n",
      "\t\u001b[31mmodified:   FinalProj2_Initial.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   FinalProj2_production.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\r\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\r\n",
      "\t\u001b[31mUntitled.ipynb\u001b[m\r\n",
      "\t\u001b[31maccum_data/\u001b[m\r\n",
      "\t\u001b[31mmlruns/\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d410365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add FinalProj2_Initial.ipynb FinalProj2_production.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afb17ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is ahead of 'origin/master' by 7 commits.\r\n",
      "  (use \"git push\" to publish your local commits)\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "\t\u001b[31mmodified:   FinalProj2_production.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\r\n",
      "\t\u001b[31mUntitled.ipynb\u001b[m\r\n",
      "\t\u001b[31maccum_data/\u001b[m\r\n",
      "\t\u001b[31mmlruns/\u001b[m\r\n",
      "\r\n",
      "no changes added to commit\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Model trained wit data at_3\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
